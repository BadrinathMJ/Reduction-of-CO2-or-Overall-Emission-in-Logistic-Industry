# -*- coding: utf-8 -*-
"""CO2 Deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UnJsfAPZKc0VCdLoITFS8Qwk5LDdgkM
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

import os
co2 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/CO2_Emissions-master/Feature Engineering CO2 Emission Final 1.xlsx')

co2.head()

"""Components of EDA

To me, there are main components of exploring data:

1)Understanding your variables

2)Cleaning your dataset

3)Analyzing relationships between variables
"""

co2.info()

co2.describe()

columns = ['Sr.No.', 'Year ', 'Jet Fuel_avi_BTU', 'Gasoline_avi_BTU',
       'LDV_SWB_road_BTU', 'LDV_LWB_road_BTU', 'Combination_Truck_road_BTU',
       'Bus_Road_BTU', 'Railways_BTU', 'Water_BTU', 'Natural_Gas_BTU',
       'LDV_SWB_EFF', 'LDV_LWB_EFF', 'Passenger_Car_EFF', 'Domestic_EFF',
       'Imported_EFF', 'Light_Truck_EFF', 'Passenger_Car_Age',
       'Light_Truck_Age', 'Light_vehicle_Age',
       'Demand_petroleum_transportation)mil_lit',
       'Average_MC/15000_miles(dollars)', 'CO2_emission_million_metric_tons',
       'CO_emission_million_shots_tons', 'NOx_emission_million_shots_tons',
       'particulate', 'SOx_emission_million_shots_tons',
       'Volatile_compound_million_shots_tons', 'Overall_emission']

"""**Handling Missing Values**"""

# Finding missing values
co2.isnull().sum()

#We will first of all check all the missing value Percentage for each of the column
features_with_na = [features for features in co2.columns if co2[features].isnull().sum()>1]
# Print the % of missing values in each feature
for feature in features_with_na:
    print(feature, np.round(co2[feature].isnull().mean(),4),'%missing values')

#Removing first and last rows as they have more na values
df = co2.iloc[1:37,:]
df

df.shape

"""**Plotting the Histogram**"""

cols = 29
rows = 38
num_cols = df.select_dtypes(exclude = 'object').columns

fig = plt.figure(figsize = (cols*3, rows*3))

for i, col in enumerate(num_cols):
    ax = fig.add_subplot(rows,cols,i+1)
    sns.histplot(x = co2[col], ax = ax)

fig.tight_layout()
plt.show()

"""**Plotting Boxplot**"""

cols = 29
rows = 38
num_cols = df.select_dtypes(exclude = 'object').columns

fig = plt.figure(figsize=(40, 40))

for i, col in enumerate(num_cols):
    ax = fig.add_subplot(rows, cols, i+1)

    sns.boxplot(co2[col], ax = ax)

fig.tight_layout()
plt.show()

for i in df.columns:
    plt.figure()
    plt.tight_layout()
    sns.set(rc={"figure.figsize":(8, 5)})
    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True)
    plt.gca().set(xlabel= i,ylabel='Frequency')
    sns.boxplot(df[i], ax=ax_box , linewidth= 1.0)
    sns.histplot(df[i], ax=ax_hist , bins = 10,kde=True)

for i in df.columns:
    plt.figure()
    plt.tight_layout()
    sns.set(rc={"figure.figsize":(8, 5)})
    f, (ax_vio, ax_hist) = plt.subplots(2, sharex=True)
    plt.gca().set(xlabel= i,ylabel='Frequency')
    sns.violinplot(df[i], ax=ax_vio , linewidth= 1.0)
    sns.histplot(df[i], ax=ax_hist , bins = 10,kde=True)

cols = 29
rows = 38
num_cols = co2.select_dtypes(exclude = 'object').columns

fig = plt.figure(figsize = (cols*3, rows*3))

for i, col in enumerate(num_cols):
    ax = fig.add_subplot(rows,cols,i+1)
    sns.swarmplot(x = co2[col], ax = ax)
    
fig.tight_layout()
plt.show()

# As the data is not normal we can continue with median imputation 

from sklearn.impute import SimpleImputer
median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
median_imputer.fit(df)
df = median_imputer.transform(df)

data = pd.DataFrame(df, columns=columns)
data

data.isnull().sum()

#All the null values are imputed so let's calculate overall emission

data['overall_emission'] = data['CO2_emission_million_metric_tons'] + data['CO_emission_million_shots_tons'] + data['NOx_emission_million_shots_tons'] + data['particulate'] +data['SOx_emission_million_shots_tons'] +data['Volatile_compound_million_shots_tons']

data.head()

#Pearson Correlation 
plt.figure(figsize=(22, 20))
cor = data.corr()
sns.heatmap(cor,annot=True, cmap = plt.cm.CMRmap_r)
plt.show()

dfCorr = data.corr()
filteredDf = dfCorr[((dfCorr >= .70) | (dfCorr <= -.70)) & (dfCorr !=1.000)]
plt.figure(figsize=(30,18))
sns.heatmap(filteredDf, annot=True, cmap= plt.cm.CMRmap_r, annot_kws={
                'fontsize': 16,
                'fontweight': 'bold',
                'fontfamily': 'serif'
            })
plt.show()

#with the follwing function we can select highly correlated features 
#it will remove the features that is correlated with anything other feature
def correlation(data, threshold):
    col_corr = set()   #set of all the names of correlated columns
    corr_matrix = data.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if (corr_matrix.iloc[i,j]) > threshold: #we interested in absolute coefficient values 
                colname = corr_matrix.columns[i]   #getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(data, 0.7)
len(set(corr_features))

corr_features

"""**We can drop the unnecessary columns from the dataset which has not any correlation with the target variable. eg. sr.no.**

**Some of the features are strongly correlated with each other but they are not affecting to the target variable such as 'Domestic_EFF','Imported_EFF', 'LDV_SWB_EFF', 'Light_Truck_Age' so we will drop these columns**

**Outlier Treatment**
"""

data.columns

columns1 = ['Sr.No.', 'Year ', 'Jet Fuel_avi_BTU', 'Gasoline_avi_BTU',
       'LDV_SWB_road_BTU', 'LDV_LWB_road_BTU', 'Combination_Truck_road_BTU',
       'Bus_Road_BTU', 'Railways_BTU', 'Water_BTU', 'Natural_Gas_BTU',
       'LDV_SWB_EFF', 'LDV_LWB_EFF', 'Passenger_Car_EFF', 'Domestic_EFF',
       'Imported_EFF', 'Light_Truck_EFF', 'Passenger_Car_Age',
       'Light_Truck_Age', 'Light_vehicle_Age',
       'Demand_petroleum_transportation)mil_lit',
       'Average_MC/15000_miles(dollars)', 'CO2_emission_million_metric_tons',
       'CO_emission_million_shots_tons', 'NOx_emission_million_shots_tons',
       'particulate', 'SOx_emission_million_shots_tons',
       'Volatile_compound_million_shots_tons', 'Overall_emission',
       'overall_emission']

!pip install feature-engine

from feature_engine.outliers import Winsorizer

IQR = data.quantile(0.75) - data.quantile(0.25)
lower_limit = data.quantile(0.25) - (IQR * 1.5)
upper_limit = data.quantile(0.75) + (IQR * 1.5)

for i in columns1:
    winsor = Winsorizer(capping_method='iqr',
                            tail='both',
                            fold=1.5,
                            variables=[i])
    data[i] = winsor.fit_transform(data[[i]])

import seaborn as sns
for feature in data:
    sns.boxplot(data[feature])
    plt.show()

data1 = data.drop({'Sr.No.','Railways_BTU', 'Water_BTU', 'Natural_Gas_BTU',
       'LDV_SWB_EFF', 'LDV_LWB_EFF','Domestic_EFF','Imported_EFF', 'Light_Truck_EFF','Light_Truck_Age', 'Light_vehicle_Age',
       'particulate', 'Overall_emission'},axis=1)

data1.head()

data1.shape

data1.columns

data2 = data1.rename({'Year ':'year'},axis = 1)
data2

# Creating a CSV file
data2.to_csv("co2_cleaned", index=False, encoding = "utf-8")

"""**Feature Scaling**

**As data is not normally distributed we use Normalization technique.**
"""

import pandas as pd
df = pd.read_csv("/content/co2_cleaned")
df.head()

df.columns

df.shape

columns2 = ['year', 'Jet Fuel_avi_BTU', 'Gasoline_avi_BTU', 'LDV_SWB_road_BTU',
       'LDV_LWB_road_BTU', 'Combination_Truck_road_BTU', 'Bus_Road_BTU',
       'Passenger_Car_EFF', 'Passenger_Car_Age',
       'Demand_petroleum_transportation)mil_lit',
       'Average_MC/15000_miles(dollars)', 'CO2_emission_million_metric_tons',
       'CO_emission_million_shots_tons', 'NOx_emission_million_shots_tons',
       'SOx_emission_million_shots_tons',
       'Volatile_compound_million_shots_tons']

from sklearn.model_selection import train_test_split

X = df.drop("overall_emission", axis=1)
y = df['overall_emission']

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.20,random_state =3)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

X_test

X_train.head()

import pickle
pickle.dump(scaler, open('scaling.pkl', 'wb'))

# Creating a CSV file
df1 = df.to_csv("emission_cleaned", index=False, encoding = "utf-8")

"""**Model Building**

## **MANUAL MODEL BUILDING**
"""

df.columns

df.rename(columns = {'Demand_petroleum_transportation)mil_lit':'Demand_Petro',"Jet Fuel_avi_BTU":"Jet_Fuel_avi_BTU",'Average_MC/15000_miles(dollars)':'Avg_MC'}, inplace=True)

# importing module
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import numpy as np

# creating an object of LinearRegression class
LR = LinearRegression()
# fitting the training data
LR.fit(X_train,y_train)

y_prediction =  LR.predict(X_test)
y_prediction

print('Train Score: ', LR.score(X_train, y_train))  
print('Test Score: ', LR.score(X_test, y_test))

# predicting the accuracy score
score=r2_score(y_test,y_prediction)
print('r2 score is ',score)
print('mean_sqrd_error is==',mean_squared_error(y_test,y_prediction))
print('root_mean_squared error of is==',np.sqrt(mean_squared_error(y_test,y_prediction)))

"""**Bayesian Ridge Regression**"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.linear_model import BayesianRidge

# Creating and training model
model_BR = BayesianRidge()
model_BR.fit(X_train, y_train)

# Model making a prediction on test data
prediction = model_BR.predict(X_test)

# Evaluation of r2 score of the model against the test set
print(f"r2 Score Of Test Set : {r2_score(y_test, prediction)}")

# Saving model to disk
pickle.dump(model_BR, open('model.pkl','wb'))

# Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))

print(model.predict([[0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,16]]))

